{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Related Work"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In recommendation systems, a very popular algorithm nowadays is collaborative filtering. Koren, Bell & Volinsky (2009) introduce a very specfic collaborative filtering technique (matrix factorization technique) which has good performance in the Netflix Prize Competition. Berenzweig, Logan, Ellis & Whitman (2003) propose to use acoustic similarities to recommend songs. Schedl, Zamani, Chen, Deldjoo & Elahi (2018) have pointed out that to solve the cold start problem and playlist continuation problem in music recommendation system, context-based models can be taken into consideration. \n",
    "\n",
    "Inspired by the works mentioned above, we use collaborative filtering techniques and content-based filtering techniques in our final model. Here are some related works that guide us to design and implement our project. We would like to give special thanks to them. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "For collaborative filtering, we build up our implicit feedback model based on the library developed by Maciej (2017)."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "For content-based filtering, we build up 3 models, one is based on emotions passed by lyrics, one is based on acoustic features and one is based on genres."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "For the lyrics-based filtering model, we learn from Kim, Schmidt, Migneco, Morton, Richardson, Scott, Speck & Turnbull (2010) that recognizing musical mood is a multiclass-multilabel classification or regression problem, where we try to annotate each music piece with a set of emotions. We can represent moods as multi-dimensional vector. In addition, we can directly use the existing music database, which has been manually annotated, to train our model."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We want to associate song lyrics with musical mood and theme. Namely, we want to classify a song's mood based on its lyrics. This website (https://xindizhao19931.wixsite.com/spotify2) teaches us to use both unsupervised method, Word2Vec, and supervised method, Long Short-Term Memory (LSTM), for language processing, sentiment analysis and predictive modeling. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "And in Chapter 9 of the textbook by Leskovec, Rajaraman and Ullman (2014), we learn to construct for each item a profile, which is a record or collection of records representing important characteristics of that item. Each profile can be viewed as a vector, whose entries are paired with a set of features. We also need to create vectors with the same components that describe the userâ€™s preferences. Then we can just calculate the cosine distance between two sets to measure the similarity of two playlists."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Logan, Kositsky & Moreno (2004) show us that we can use latent semantic analysis to compare the similarities among lyrics. However, as the paper points out, \"similarity based on lyrics was found to be better than random but inferior to acoustic similarity, at least for the ground truth used.\" "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Finally, we learn how to prepare our training dataset more appropriately from the work of Yang, Jeong, Choi & Lee (2018). We would like to filter out songs that are not so popular, and therefore appear in only one or two playlists."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "[1] Koren, Yehuda, Robert Bell, and Chris Volinsky. \"Matrix factorization techniques for recommender systems.\" Computer 42.8 (2009).\n",
    "\n",
    "[2] Shedl, M. et al, Current Challenges and Visions in Music Recommender Systems Research, https://arxiv.org/pdf/1710.03208.pdf. \n",
    "\n",
    "[3] Berenzweig, Adam, Beth Logan, Daniel P.W. Ellis and Brian Whitman. A Large-Scale Evaluation of Acoustic and Subjective Music Similarity Measures. Proceedings of the ISMIR International Conference on Music Information Retrieval (Baltimore, MD), 2003, pp. 99?105. \n",
    "\n",
    "[4] Kula Maciej. \"Spotlight.\" (2017)\n",
    "https://github.com/maciejkula/spotlight\n",
    "\n",
    "[5] Kim, Y.E., Schmidt, E.M., Migneco, R., Morton, B.G., Richardson, P., Scott, J., Speck, J.A. and Turnbull, D., 2010, August. Music emotion recognition: A state of the art review. In Proc. ISMIR (pp. 255-266).\n",
    "\n",
    "[6] Leskovec, J., Rajaraman, A. and Ullman, J.D., 2014. Mining of massive datasets. Cambridge university press.\n",
    "\n",
    "[7] Logan, B., Kositsky, A. and Moreno, P., 2004, June. Semantic analysis of song lyrics. In Multimedia and Expo, 2004. ICME'04. 2004 IEEE International Conference on (Vol. 2, pp. 827-830). IEEE.\n",
    "\n",
    "[8] Yang, H., Jeong, Y., Choi, M. and Lee, J., 2018, October. MMCF: Multimodal Collaborative Filtering for Automatic Playlist Continuation. In Proceedings of the ACM Recommender Systems Challenge 2018 (p. 11). ACM."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
