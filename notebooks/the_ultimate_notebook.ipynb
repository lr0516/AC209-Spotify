{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Content-based Recommendation"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Part1: profiling the tracks"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In the first part we try to extract features that represents the content of a certain track. We tried two kind of features: lyrics features and audio features. Finally, the lyrics of a song is reduced to a 4-entry profile and the audio features of a song is reduced to a 9-entry profile."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Part1.1: baseline model for emotion recognition"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In this part we looked for a way to represent and model the lyrics so that it can be used to predict emotion of a track. We compared two representations: bag of words + term frequency inverse document frequency vs. raw lyrics + feature hashing. We used these two representations as input to train a classifier that differentiate happy songs and sad songs and compared the performance of different classification models (SVM, random forest, naive Bayes, logistic regression). We finally chose raw lyrics + feature hasing + random forest as our baseline model."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "%matplotlib inline\n",
    "import sqlite3\n",
    "import json\n",
    "from scipy.sparse                    import csr_matrix, vstack\n",
    "from sklearn.feature_extraction.text import TfidfTransformer, HashingVectorizer\n",
    "from sklearn.svm                     import SVC\n",
    "from sklearn.ensemble                import RandomForestClassifier\n",
    "from sklearn.naive_bayes             import GaussianNB\n",
    "from sklearn.linear_model            import LogisticRegression"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "MXM_PATH  = 'databases/mxm_dataset.db'\n",
    "TAG_PATH  = 'databases/lastfm_tags.db'\n",
    "META_PATH = 'databases/track_metadata.db'\n",
    "WIKI_PATH = 'databases/lyricwiki.db'\n",
    "\n",
    "conn_mxm  = sqlite3.connect(MXM_PATH)\n",
    "conn_tag  = sqlite3.connect(TAG_PATH)\n",
    "conn_meta = sqlite3.connect(META_PATH)\n",
    "conn_wiki = sqlite3.connect(WIKI_PATH)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### BOW + TF-IDF"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\"Get the list of 5000 words used for musiXmatch's BOW representation\"\"\"\n",
    "def get_mxm_vocab(conn_mxm):\n",
    "    sql  = \"SELECT * FROM words\"\n",
    "    res  = conn_mxm.execute(sql)\n",
    "    data = res.fetchall()\n",
    "    mxm_vocab = [t[0] for t in data]\n",
    "    return mxm_vocab"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "mxm_vocab = get_mxm_vocab(conn_mxm)\n",
    "mxm_dict  = {mxm_vocab[i] : i for i in range(len(mxm_vocab))}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\"Get the BOW regresentation of the tracks in the form of a sparse matrix\"\"\"\n",
    "def get_bagofwords(tids, mxm_dict, conn_mxm):\n",
    "    bows = []\n",
    "    for tid in tids:\n",
    "        sql  = \"SELECT word, count FROM lyrics WHERE track_id='{}'\".format(tid)\n",
    "        res  = conn_mxm.execute(sql)\n",
    "        data = res.fetchall()\n",
    "        col  = np.array([mxm_dict[t[0]] for t in data], dtype=np.int16)\n",
    "        row  = np.zeros(len(col),                       dtype=np.int16)\n",
    "        cnt  = np.array([t[1] for t in data] )\n",
    "        bow  = csr_matrix((cnt, (row, col)), shape=(1, 5000))\n",
    "        bows.append(bow)\n",
    "    return vstack(bows)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\"Get the track ids. tagged with a certain tag\"\"\"\n",
    "def get_tids_oftag(tag, conn_tag):\n",
    "    sql  = \"\"\"SELECT tids.tid FROM tid_tag, tids, tags \n",
    "              WHERE tids.ROWID=tid_tag.tid AND tid_tag.tag=tags.ROWID \n",
    "              AND tags.tag='{}'\"\"\".format(tag)\n",
    "    res  = conn_tag.execute(sql)\n",
    "    data = res.fetchall()\n",
    "    return [t[0] for t in data]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Get the BOW representation for tracks labelled with 'happy' and 'sad'.\n",
    "happy_tids = get_tids_oftag('happy', conn_tag)\n",
    "happy_bows = get_bagofwords(happy_tids, mxm_dict, conn_mxm)\n",
    "sad_tids   = get_tids_oftag('sad',   conn_tag)\n",
    "sad_bows   = get_bagofwords(sad_tids, mxm_dict,   conn_mxm)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(24716, 5000)"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Do the term frequency - inverse document frequency transformation.\n",
    "all_bows = vstack([happy_bows, sad_bows])\n",
    "tfidf = TfidfTransformer()\n",
    "all_bows_tfidf = tfidf.fit_transform(all_bows)\n",
    "all_bows_tfidf.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "X = all_bows_tfidf.copy().toarray()\n",
    "y = ['happy']*(happy_bows.shape[0]) + \\\n",
    "    ['sad']*(sad_bows.shape[0])\n",
    "y = np.array([1 if t == 'happy' else 0 for t in y])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "nonzeros = np.sum(X, axis=1) != 0.0\n",
    "X_nonzero = X[nonzeros]\n",
    "y_nonzero = y[nonzeros]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train, X_test, y_train, y_test = train_test_split(\n",
    "    X_nonzero, y_nonzero, test_size=0.5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "RandomForestClassifier(bootstrap=True, class_weight=None, criterion='gini',\n",
       "            max_depth=None, max_features=70, max_leaf_nodes=None,\n",
       "            min_impurity_decrease=0.0, min_impurity_split=None,\n",
       "            min_samples_leaf=1, min_samples_split=2,\n",
       "            min_weight_fraction_leaf=0.0, n_estimators=300, n_jobs=None,\n",
       "            oob_score=False, random_state=None, verbose=0,\n",
       "            warm_start=False)"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "forest = RandomForestClassifier(\n",
    "    n_estimators=300, max_features=70, max_depth=None, min_samples_split=2)\n",
    "forest.fit(X_train, y_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.984089845577913"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "forest.score(X_train, y_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.6620399251403618"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "forest.score(X_test, y_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "GaussianNB(priors=None, var_smoothing=1e-09)"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "naive = GaussianNB()\n",
    "naive.fit(X_train, y_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.7622835751052878"
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "naive.score(X_train, y_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.5920149719276356"
      ]
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "naive.score(X_test, y_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/chenfeiyu/anaconda3/lib/python3.6/site-packages/sklearn/linear_model/logistic.py:432: FutureWarning: Default solver will be changed to 'lbfgs' in 0.22. Specify a solver to silence this warning.\n",
      "  FutureWarning)\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "LogisticRegression(C=1.0, class_weight=None, dual=False, fit_intercept=True,\n",
       "          intercept_scaling=1, max_iter=100, multi_class='warn',\n",
       "          n_jobs=None, penalty='l2', random_state=None, solver='warn',\n",
       "          tol=0.0001, verbose=0, warm_start=False)"
      ]
     },
     "execution_count": 18,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "logreg = LogisticRegression()\n",
    "logreg.fit(X_train, y_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.7719544532834192"
      ]
     },
     "execution_count": 19,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "logreg.score(X_train, y_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.6781035558328135"
      ]
     },
     "execution_count": 20,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "logreg.score(X_test, y_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "girl\n",
      "got\n",
      "danc\n",
      "gonna\n",
      "get\n",
      "happi\n",
      "rock\n",
      "up\n",
      "about\n",
      "citi\n",
      "fine\n",
      "shine\n",
      "doo\n",
      "jump\n",
      "fun\n",
      "kiss\n",
      "hot\n",
      "sky\n",
      "check\n",
      "ya\n"
     ]
    }
   ],
   "source": [
    "# Let's see the top 20 words that indicate happiness\n",
    "top = np.argsort(logreg.coef_[0])[::-1]\n",
    "for i in top[:20]: \n",
    "    print(mxm_vocab[i])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/chenfeiyu/anaconda3/lib/python3.6/site-packages/sklearn/svm/base.py:196: FutureWarning: The default value of gamma will change from 'auto' to 'scale' in version 0.22 to account better for unscaled features. Set gamma explicitly to 'auto' or 'scale' to avoid this warning.\n",
      "  \"avoid this warning.\", FutureWarning)\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "SVC(C=1.0, cache_size=200, class_weight=None, coef0=0.0,\n",
       "  decision_function_shape='ovr', degree=3, gamma='auto_deprecated',\n",
       "  kernel='rbf', max_iter=-1, probability=False, random_state=None,\n",
       "  shrinking=True, tol=0.001, verbose=False)"
      ]
     },
     "execution_count": 21,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "svc = SVC()\n",
    "svc.fit(X_train, y_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.5484323818437061"
      ]
     },
     "execution_count": 22,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "svc.score(X_train, y_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.5483468496568933"
      ]
     },
     "execution_count": 23,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "svc.score(X_test, y_test)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### raw lyrics + feature hashing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\"Get the artist and title for tracks with tids\"\"\"\n",
    "def get_artistsandtitles(tids, conn_meta):\n",
    "    AandTs = []\n",
    "    for tid in tids:\n",
    "        sql = \"SELECT artist_name, title FROM songs WHERE track_id='{}'\".format(tid)\n",
    "        res = conn_meta.execute(sql)\n",
    "        AandTs.append(res.fetchall()[0])\n",
    "    return AandTs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "\"\"\"Get the raw lyrics of songs with certain artist and title\"\"\"\n",
    "def get_lyrics(AandTs, conn_wiki):\n",
    "    queryconds = [(t[0].lower().strip(), t[1].lower().strip()) for t in AandTs]\n",
    "    lyricslist = []\n",
    "    total_found = 0\n",
    "    for n, t in enumerate(queryconds):\n",
    "        res = conn_wiki.execute(\"\"\"SELECT lyrics FROM songs WHERE artist=? AND title=?\"\"\", (t[0], t[1]))\n",
    "        try:\n",
    "            lyricslist.append(res.fetchall()[0][0])\n",
    "            total_found = total_found + 1\n",
    "        except:\n",
    "            continue\n",
    "    return lyricslist"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [],
   "source": [
    "happy_AandTs = get_artistsandtitles(happy_tids, conn_meta)\n",
    "happy_lyrics = get_lyrics(happy_AandTs, conn_wiki)\n",
    "sad_AandTs   = get_artistsandtitles(sad_tids,   conn_meta)\n",
    "sad_lyrics   = get_lyrics(sad_AandTs,   conn_wiki)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_nonempty(lyrics):\n",
    "    nonempty = []\n",
    "    for l in lyrics:\n",
    "        if len(l) > 0:\n",
    "            nonempty.append(l)\n",
    "    return nonempty"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [],
   "source": [
    "happy_lyrics_nonempty = get_nonempty(happy_lyrics)\n",
    "sad_lyrics_nonempty   = get_nonempty(sad_lyrics)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [],
   "source": [
    "vectorizer = HashingVectorizer(n_features=1000)\n",
    "anotherX   = vectorizer.fit_transform(\n",
    "    happy_lyrics_nonempty+sad_lyrics_nonempty)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [],
   "source": [
    "anothery = ['happy']*(len(happy_lyrics_nonempty)) \\\n",
    "    + ['sad']*(len(sad_lyrics_nonempty))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [],
   "source": [
    "anotherX_train, anotherX_test, anothery_train, anothery_test = train_test_split(\n",
    "    anotherX, anothery, test_size=0.5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "RandomForestClassifier(bootstrap=True, class_weight=None, criterion='gini',\n",
       "            max_depth=None, max_features=30, max_leaf_nodes=None,\n",
       "            min_impurity_decrease=0.0, min_impurity_split=None,\n",
       "            min_samples_leaf=1, min_samples_split=2,\n",
       "            min_weight_fraction_leaf=0.0, n_estimators=300, n_jobs=None,\n",
       "            oob_score=False, random_state=None, verbose=0,\n",
       "            warm_start=False)"
      ]
     },
     "execution_count": 33,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "anotherforest = RandomForestClassifier(\n",
    "    n_estimators=300, max_features=30, max_depth=None, min_samples_split=2)\n",
    "anotherforest.fit(anotherX_train, anothery_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.9799670044779637"
      ]
     },
     "execution_count": 34,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "anotherforest.score(anotherX_train, anothery_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.7509131613055261"
      ]
     },
     "execution_count": 35,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "anotherforest.score(anotherX_test, anothery_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "GaussianNB(priors=None, var_smoothing=1e-09)"
      ]
     },
     "execution_count": 36,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "anothernaive = GaussianNB()\n",
    "anothernaive.fit(anotherX_train.toarray(), anothery_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.6586141880744756"
      ]
     },
     "execution_count": 37,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "anothernaive.score(anotherX_train.toarray(), anothery_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.615411806291976"
      ]
     },
     "execution_count": 38,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "anothernaive.score(anotherX_test.toarray(), anothery_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/chenfeiyu/anaconda3/lib/python3.6/site-packages/sklearn/linear_model/logistic.py:432: FutureWarning: Default solver will be changed to 'lbfgs' in 0.22. Specify a solver to silence this warning.\n",
      "  FutureWarning)\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "LogisticRegression(C=1.0, class_weight=None, dual=False, fit_intercept=True,\n",
       "          intercept_scaling=1, max_iter=100, multi_class='warn',\n",
       "          n_jobs=None, penalty='l2', random_state=None, solver='warn',\n",
       "          tol=0.0001, verbose=0, warm_start=False)"
      ]
     },
     "execution_count": 39,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "anotherlogreg = LogisticRegression()\n",
    "anotherlogreg.fit(anotherX_train, anothery_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.7231911383455103"
      ]
     },
     "execution_count": 40,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "anotherlogreg.score(anotherX_train, anothery_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.6849298927771886"
      ]
     },
     "execution_count": 41,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "anotherlogreg.score(anotherX_test, anothery_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/chenfeiyu/anaconda3/lib/python3.6/site-packages/sklearn/svm/base.py:196: FutureWarning: The default value of gamma will change from 'auto' to 'scale' in version 0.22 to account better for unscaled features. Set gamma explicitly to 'auto' or 'scale' to avoid this warning.\n",
      "  \"avoid this warning.\", FutureWarning)\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "SVC(C=1.0, cache_size=200, class_weight=None, coef0=0.0,\n",
       "  decision_function_shape='ovr', degree=3, gamma='auto_deprecated',\n",
       "  kernel='rbf', max_iter=-1, probability=False, random_state=None,\n",
       "  shrinking=True, tol=0.001, verbose=False)"
      ]
     },
     "execution_count": 42,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "anothersvc = SVC()\n",
    "anothersvc.fit(anotherX_train, anothery_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.5136695734150365"
      ]
     },
     "execution_count": 43,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "anothersvc.score(anotherX_train, anothery_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.5167903852951573"
      ]
     },
     "execution_count": 44,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "anothersvc.score(anotherX_test, anothery_test)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Part1.2: comparing LSTM with baseline model for emotion recognition"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "to gain a better classification accuracy, we tried more complex models and representation. Here we compared the performance of word2vec+LSTM against our baseline model. We found that our baseline model out performed this new model in all cases. So we chose our baseline model as our final model for lyrics-based emotion recognition."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Training set and Validation set for comparison"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import numpy as np\n",
    "%matplotlib inline\n",
    "import sqlite3\n",
    "from sklearn.model_selection import train_test_split\n",
    "from langdetect              import detect"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\"Filter out non-English lyrics\"\"\"\n",
    "def get_english(lyrics):\n",
    "    english = []\n",
    "    for l in lyrics:\n",
    "        try:\n",
    "            if detect(l) == 'en':\n",
    "                english.append(l)\n",
    "        except: continue\n",
    "    return english"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\"Create training set and validation for bi-polar tags.\"\"\"\n",
    "def create_train_test(tag1, tag2):\n",
    "    print('Collecting tids...')\n",
    "    tag1_tids = get_tids_oftag(tag1, conn_tag)\n",
    "    tag2_tids = get_tids_oftag(tag2, conn_tag)\n",
    "    print('Collecting artists and titles...')\n",
    "    tag1_AandTs = get_artistsandtitles(tag1_tids, conn_meta)\n",
    "    tag2_AandTs = get_artistsandtitles(tag2_tids, conn_meta)\n",
    "    print('Collecting lyrics...')\n",
    "    tag1_lyrics = get_lyrics(tag1_AandTs, conn_wiki)\n",
    "    tag2_lyrics = get_lyrics(tag2_AandTs, conn_wiki)\n",
    "    print('{} tracks collected for tag {}, {} tracks collected for tag {}'.format(\\\n",
    "          len(tag1_lyrics), tag1, len(tag2_lyrics), tag2))\n",
    "    print('Filtering out empty lyrics...')\n",
    "    tag1_lyrics_nonempty = get_nonempty(tag1_lyrics)\n",
    "    tag2_lyrics_nonempty = get_nonempty(tag2_lyrics)\n",
    "    print('Filtering out non-English lyrics...')\n",
    "    tag1_lyrics_nonempty = get_english(tag1_lyrics_nonempty)\n",
    "    tag2_lyrics_nonempty = get_english(tag2_lyrics_nonempty)\n",
    "    print('{} nonempty lyrics for tag {}, {} nonempty lyrics for tag {}'.format(\\\n",
    "          len(tag1_lyrics_nonempty), tag1, len(tag2_lyrics_nonempty), tag2))\n",
    "    print('Creating predictor set and target set...')\n",
    "    X = np.array(        tag1_lyrics_nonempty  +         tag2_lyrics_nonempty )\n",
    "    y = np.array([1]*len(tag1_lyrics_nonempty) + [0]*len(tag2_lyrics_nonempty))\n",
    "    print('Splitting training set and validation set...')\n",
    "    X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2)\n",
    "    print('writing to disk...')\n",
    "    dirname = tag1+'-'+tag2\n",
    "    os.mkdir(dirname)\n",
    "    np.save(dirname + '/X_train.npy', X_train)\n",
    "    np.save(dirname + '/X_test.npy',  X_test)\n",
    "    np.save(dirname + '/y_train.npy', y_train)\n",
    "    np.save(dirname + '/y_test.npy',  y_test)\n",
    "    print('Finished.')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Collecting tids...\n",
      "Collecting artists and titles...\n",
      "Collecting lyrics...\n",
      "8508 tracks collected for tag happy, 9078 tracks collected for tag sad\n",
      "Filtering out empty lyrics...\n",
      "Filtering out non-English lyrics...\n",
      "7724 nonempty lyrics for tag happy, 8300 nonempty lyrics for tag sad\n",
      "Creating predictor set and target set...\n",
      "Splitting training set and validation set...\n",
      "writing to disk...\n",
      "Finished.\n"
     ]
    }
   ],
   "source": [
    "create_train_test('happy', 'sad')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Collecting tids...\n",
      "Collecting artists and titles...\n",
      "Collecting lyrics...\n",
      "6287 tracks collected for tag relax, 4521 tracks collected for tag Energetic\n",
      "Filtering out empty lyrics...\n",
      "Filtering out non-English lyrics...\n",
      "5213 nonempty lyrics for tag relax, 4068 nonempty lyrics for tag Energetic\n",
      "Creating predictor set and target set...\n",
      "Splitting training set and validation set...\n",
      "writing to disk...\n",
      "Finished.\n"
     ]
    }
   ],
   "source": [
    "create_train_test('relax', 'Energetic')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Collecting tids...\n",
      "Collecting artists and titles...\n",
      "Collecting lyrics...\n",
      "12457 tracks collected for tag cool, 12769 tracks collected for tag oldies\n",
      "Filtering out empty lyrics...\n",
      "Filtering out non-English lyrics...\n",
      "11064 nonempty lyrics for tag cool, 11970 nonempty lyrics for tag oldies\n",
      "Creating predictor set and target set...\n",
      "Splitting training set and validation set...\n",
      "writing to disk...\n",
      "Finished.\n"
     ]
    }
   ],
   "source": [
    "create_train_test('cool', 'oldies')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Collecting tids...\n",
      "Collecting artists and titles...\n",
      "Collecting lyrics...\n",
      "5169 tracks collected for tag sweet, 4768 tracks collected for tag dark\n",
      "Filtering out empty lyrics...\n",
      "Filtering out non-English lyrics...\n",
      "4773 nonempty lyrics for tag sweet, 4228 nonempty lyrics for tag dark\n",
      "Creating predictor set and target set...\n",
      "Splitting training set and validation set...\n",
      "writing to disk...\n",
      "Finished.\n"
     ]
    }
   ],
   "source": [
    "create_train_test('sweet', 'dark')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Performance of baseline model (feature hashing + random forest) vs new model (word2vec + LSTM)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from keras.models                    import Sequential\n",
    "from keras.layers                    import Dense, Conv1D, MaxPooling1D, LSTM, Dropout\n",
    "from keras.layers.embeddings         import Embedding\n",
    "from keras.preprocessing             import sequence"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def train_forest(tag1, tag2):\n",
    "    dirname = tag1 + '-' + tag2\n",
    "    vectorizer = HashingVectorizer(n_features=1000)\n",
    "    \n",
    "    print('Preprocessing training set...')\n",
    "    X_train_forest = vectorizer.fit_transform(np.load(dirname + '/X_train.npy'))\n",
    "    y_train_forest = np.load(dirname + '/y_train.npy')\n",
    "    print('Preprocessing validation set...')\n",
    "    X_test_forest  = vectorizer.transform(np.load(dirname + '/X_test.npy'))\n",
    "    y_test_forest  = np.load(dirname + '/y_test.npy')\n",
    "    print('Training random forest...')\n",
    "    forest = RandomForestClassifier(\n",
    "    n_estimators=300, max_features=30, max_depth=None, min_samples_split=2)\n",
    "    forest.fit(X_train_forest, y_train_forest)\n",
    "    print('Finished.')\n",
    "    return forest, vectorizer, X_train_forest, y_train_forest, X_test_forest, y_test_forest"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def show_performance(res):\n",
    "    print('Training accuracy:', res[0].score(res[2], res[3]))\n",
    "    print('Validation accuracy:', res[0].score(res[4], res[5]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "def lyrics2intarray(lyrics, tokenizer, stemmer):\n",
    "    lyrics_tokenized = tokenizer.tokenize(lyrics)\n",
    "    lyrics_stemmed   = [stemmer.stem(w) for w in lyrics_tokenized]\n",
    "    L = len(lyrics_stemmed)\n",
    "    intarray = np.zeros(L, dtype=np.int32)\n",
    "    for i in range(L):\n",
    "        intarray[i] = mxm_dict.get(lyrics_stemmed[i], 0)\n",
    "    return intarray"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "def train_lstm(tag1, tag2):\n",
    "    stemmer = SnowballStemmer('english')\n",
    "    tokenizer = RegexpTokenizer(r'\\w+')\n",
    "    dirname = tag1 + '-' + tag2\n",
    "    print('Preprocessing training set...')\n",
    "    X_train_lstm = [lyrics2intarray(l, tokenizer, stemmer) for l in np.load(dirname + '/X_train.npy')]\n",
    "    y_train_lstm = np.load(dirname + '/y_train.npy')\n",
    "    print('Preprocessing validation set...')\n",
    "    X_test_lstm  = [lyrics2intarray(l, tokenizer, stemmer) for l in np.load(dirname + '/X_test.npy')]\n",
    "    y_test_lstm  = np.load(dirname + '/y_test.npy')\n",
    "    print('Trimming input...')\n",
    "    max_lyrics_length = 150\n",
    "    X_train_lstm = sequence.pad_sequences(X_train_lstm, maxlen=max_lyrics_length)\n",
    "    X_test_lstm  = sequence.pad_sequences(X_test_lstm,  maxlen=max_lyrics_length)\n",
    "    print('Training LSTM...')\n",
    "    embedding_vecor_length = 64\n",
    "    model = Sequential()\n",
    "    model.add(Embedding(5001, embedding_vecor_length, input_length=max_lyrics_length))\n",
    "    model.add(Dropout(0.5))\n",
    "    model.add(Conv1D(filters=64, kernel_size=3, padding='same', activation='relu'))\n",
    "    model.add(MaxPooling1D(pool_size=2))\n",
    "    model.add(Dropout(0.5))\n",
    "    model.add(LSTM(50))\n",
    "    model.add(Dropout(0.5))\n",
    "    model.add(Dense(1, activation='sigmoid'))\n",
    "    model.compile(loss='binary_crossentropy', optimizer='adam', metrics=['accuracy'])\n",
    "    print(model.summary())\n",
    "    model.fit(X_train_lstm, y_train_lstm, validation_data=(X_test_lstm, y_test_lstm), epochs=5, batch_size=64)\n",
    "    return model, X_train_lstm, y_train_lstm, X_test_lstm, y_test_lstm"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Preprocessing training set...\n",
      "Preprocessing validation set...\n",
      "Training random forest...\n",
      "Finished.\n",
      "Training accuracy: 0.9726187690147438\n",
      "Validation accuracy: 0.7759750390015601\n"
     ]
    }
   ],
   "source": [
    "show_performance(train_forest('happy', 'sad'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Preprocessing training set...\n",
      "Preprocessing validation set...\n",
      "Trimming input...\n",
      "Training LSTM...\n",
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "embedding_1 (Embedding)      (None, 150, 64)           320064    \n",
      "_________________________________________________________________\n",
      "dropout_1 (Dropout)          (None, 150, 64)           0         \n",
      "_________________________________________________________________\n",
      "conv1d_1 (Conv1D)            (None, 150, 64)           12352     \n",
      "_________________________________________________________________\n",
      "max_pooling1d_1 (MaxPooling1 (None, 75, 64)            0         \n",
      "_________________________________________________________________\n",
      "dropout_2 (Dropout)          (None, 75, 64)            0         \n",
      "_________________________________________________________________\n",
      "lstm_1 (LSTM)                (None, 50)                23000     \n",
      "_________________________________________________________________\n",
      "dropout_3 (Dropout)          (None, 50)                0         \n",
      "_________________________________________________________________\n",
      "dense_1 (Dense)              (None, 1)                 51        \n",
      "=================================================================\n",
      "Total params: 355,467\n",
      "Trainable params: 355,467\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n",
      "None\n",
      "Train on 12819 samples, validate on 3205 samples\n",
      "Epoch 1/5\n",
      "12819/12819 [==============================] - 21s 2ms/step - loss: 0.6661 - acc: 0.5842 - val_loss: 0.5918 - val_acc: 0.6871\n",
      "Epoch 2/5\n",
      "12819/12819 [==============================] - 20s 2ms/step - loss: 0.5624 - acc: 0.7176 - val_loss: 0.5645 - val_acc: 0.7236\n",
      "Epoch 3/5\n",
      "12819/12819 [==============================] - 20s 2ms/step - loss: 0.5041 - acc: 0.7664 - val_loss: 0.5555 - val_acc: 0.7310\n",
      "Epoch 4/5\n",
      "12819/12819 [==============================] - 21s 2ms/step - loss: 0.4722 - acc: 0.7859 - val_loss: 0.5509 - val_acc: 0.7282\n",
      "Epoch 5/5\n",
      "12819/12819 [==============================] - 20s 2ms/step - loss: 0.4455 - acc: 0.7992 - val_loss: 0.5492 - val_acc: 0.7351\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "(<keras.engine.sequential.Sequential at 0x1103d89b0>,\n",
       " array([[   0,    0,    0, ...,   25,   14, 1112],\n",
       "        [ 141,    2, 1318, ...,   59,    2,  513],\n",
       "        [  92,    3,   16, ...,  107,   10, 2921],\n",
       "        ...,\n",
       "        [ 134,   35,  169, ...,    3,    4,   29],\n",
       "        [   0,    0,    0, ...,   13,   27,    7],\n",
       "        [1001,  245,  503, ...,  119,   13,  154]], dtype=int32),\n",
       " array([0, 0, 1, ..., 1, 1, 0]),\n",
       " array([[   0,    2,    0, ...,    7,   37,  941],\n",
       "        [ 880,    3,    3, ...,  545, 1748, 1543],\n",
       "        [  14,  542,   46, ...,   15,    3,   16],\n",
       "        ...,\n",
       "        [   0,    0,    0, ..., 1803,   11,  569],\n",
       "        [  12,    9,    2, ...,  416,  264,  200],\n",
       "        [  61,   36,   30, ...,  171, 1222,  125]], dtype=int32),\n",
       " array([0, 0, 0, ..., 1, 0, 1]))"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train_lstm('happy', 'sad')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Preprocessing training set...\n",
      "Preprocessing validation set...\n",
      "Training random forest...\n",
      "Finished.\n",
      "Training accuracy: 0.9888200431034483\n",
      "Validation accuracy: 0.7926763597199784\n"
     ]
    }
   ],
   "source": [
    "show_performance(train_forest('relax', 'Energetic'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Preprocessing training set...\n",
      "Preprocessing validation set...\n",
      "Trimming input...\n",
      "Training LSTM...\n",
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "embedding_2 (Embedding)      (None, 150, 64)           320064    \n",
      "_________________________________________________________________\n",
      "dropout_4 (Dropout)          (None, 150, 64)           0         \n",
      "_________________________________________________________________\n",
      "conv1d_2 (Conv1D)            (None, 150, 64)           12352     \n",
      "_________________________________________________________________\n",
      "max_pooling1d_2 (MaxPooling1 (None, 75, 64)            0         \n",
      "_________________________________________________________________\n",
      "dropout_5 (Dropout)          (None, 75, 64)            0         \n",
      "_________________________________________________________________\n",
      "lstm_2 (LSTM)                (None, 50)                23000     \n",
      "_________________________________________________________________\n",
      "dropout_6 (Dropout)          (None, 50)                0         \n",
      "_________________________________________________________________\n",
      "dense_2 (Dense)              (None, 1)                 51        \n",
      "=================================================================\n",
      "Total params: 355,467\n",
      "Trainable params: 355,467\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n",
      "None\n",
      "Train on 7424 samples, validate on 1857 samples\n",
      "Epoch 1/5\n",
      "7424/7424 [==============================] - 13s 2ms/step - loss: 0.6794 - acc: 0.5672 - val_loss: 0.6409 - val_acc: 0.6462\n",
      "Epoch 2/5\n",
      "7424/7424 [==============================] - 12s 2ms/step - loss: 0.5830 - acc: 0.6989 - val_loss: 0.5570 - val_acc: 0.7264\n",
      "Epoch 3/5\n",
      "7424/7424 [==============================] - 12s 2ms/step - loss: 0.4871 - acc: 0.7736 - val_loss: 0.5433 - val_acc: 0.7329\n",
      "Epoch 4/5\n",
      "7424/7424 [==============================] - 12s 2ms/step - loss: 0.4351 - acc: 0.8110 - val_loss: 0.5338 - val_acc: 0.7485\n",
      "Epoch 5/5\n",
      "7424/7424 [==============================] - 12s 2ms/step - loss: 0.3794 - acc: 0.8450 - val_loss: 0.5442 - val_acc: 0.7474\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "(<keras.engine.sequential.Sequential at 0x1a319659e8>,\n",
       " array([[  83,    1,   61, ...,  135,    3,   37],\n",
       "        [  28, 3422,    2, ...,  976,  523,  172],\n",
       "        [   0,    0,    0, ...,  254,   59,  390],\n",
       "        ...,\n",
       "        [ 267,    4,  251, ...,  336,    6,   92],\n",
       "        [  53,  476,   28, ...,   65,  175,  175],\n",
       "        [1298,    2, 1950, ...,  312,   46,   46]], dtype=int32),\n",
       " array([0, 1, 0, ..., 1, 0, 0]),\n",
       " array([[   0,    0,    0, ...,  147,  131,    3],\n",
       "        [  48,  103,  476, ...,  885,    0,  106],\n",
       "        [ 687, 1109,   13, ...,   15,    3,   84],\n",
       "        ...,\n",
       "        [4717,  103,  476, ...,  539,   13, 1685],\n",
       "        [ 399,    1, 1362, ...,   30,   12,  399],\n",
       "        [   0,  135,    0, ...,    0,  336, 1672]], dtype=int32),\n",
       " array([1, 0, 1, ..., 1, 0, 0]))"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train_lstm('relax', 'Energetic')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Preprocessing training set...\n",
      "Preprocessing validation set...\n",
      "Training random forest...\n",
      "Finished.\n",
      "Training accuracy: 0.9468171704563955\n",
      "Validation accuracy: 0.7803342739309747\n"
     ]
    }
   ],
   "source": [
    "show_performance(train_forest('cool', 'oldies'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Preprocessing training set...\n",
      "Preprocessing validation set...\n",
      "Trimming input...\n",
      "Training LSTM...\n",
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "embedding_4 (Embedding)      (None, 150, 64)           320064    \n",
      "_________________________________________________________________\n",
      "dropout_10 (Dropout)         (None, 150, 64)           0         \n",
      "_________________________________________________________________\n",
      "conv1d_4 (Conv1D)            (None, 150, 64)           12352     \n",
      "_________________________________________________________________\n",
      "max_pooling1d_4 (MaxPooling1 (None, 75, 64)            0         \n",
      "_________________________________________________________________\n",
      "dropout_11 (Dropout)         (None, 75, 64)            0         \n",
      "_________________________________________________________________\n",
      "lstm_4 (LSTM)                (None, 50)                23000     \n",
      "_________________________________________________________________\n",
      "dropout_12 (Dropout)         (None, 50)                0         \n",
      "_________________________________________________________________\n",
      "dense_4 (Dense)              (None, 1)                 51        \n",
      "=================================================================\n",
      "Total params: 355,467\n",
      "Trainable params: 355,467\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n",
      "None\n",
      "Train on 18427 samples, validate on 4607 samples\n",
      "Epoch 1/5\n",
      "18427/18427 [==============================] - 25s 1ms/step - loss: 0.6615 - acc: 0.5960 - val_loss: 0.5832 - val_acc: 0.7076\n",
      "Epoch 2/5\n",
      "18427/18427 [==============================] - 23s 1ms/step - loss: 0.5534 - acc: 0.7349 - val_loss: 0.5480 - val_acc: 0.7374\n",
      "Epoch 3/5\n",
      "18427/18427 [==============================] - 24s 1ms/step - loss: 0.5073 - acc: 0.7686 - val_loss: 0.5272 - val_acc: 0.7543\n",
      "Epoch 4/5\n",
      "18427/18427 [==============================] - 24s 1ms/step - loss: 0.4727 - acc: 0.7963 - val_loss: 0.5274 - val_acc: 0.7552\n",
      "Epoch 5/5\n",
      "18427/18427 [==============================] - 24s 1ms/step - loss: 0.4386 - acc: 0.8124 - val_loss: 0.5279 - val_acc: 0.7599\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "(<keras.engine.sequential.Sequential at 0x1a3a6c96a0>,\n",
       " array([[ 228,  312,   73, ...,  577,   66,  399],\n",
       "        [  76,  976,  523, ...,  545,   37,  588],\n",
       "        [ 427,  373,    7, ...,  525,    4,    7],\n",
       "        ...,\n",
       "        [1234,   89,    2, ...,    1,   54,  487],\n",
       "        [   8,  615,    8, ...,    8,  615,    8],\n",
       "        [ 326,    2,   85, ...,  686,    4,  125]], dtype=int32),\n",
       " array([0, 0, 1, ..., 0, 1, 1]),\n",
       " array([[  48, 1120,  211, ...,   94,    4,   66],\n",
       "        [  46,   46,   46, ...,  989,   80,  434],\n",
       "        [  88,   72,   22, ...,  105,    0,    0],\n",
       "        ...,\n",
       "        [1110,  390,    7, ...,  616,  180,    3],\n",
       "        [   7,  156,    7, ...,  335,    4,   44],\n",
       "        [   0,    0,    0, ...,   76,    2,  358]], dtype=int32),\n",
       " array([1, 0, 1, ..., 1, 0, 0]))"
      ]
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train_lstm('cool', 'oldies')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Preprocessing training set...\n",
      "Preprocessing validation set...\n",
      "Training random forest...\n",
      "Finished.\n",
      "Training accuracy: 0.9870833333333333\n",
      "Validation accuracy: 0.7679067184897279\n"
     ]
    }
   ],
   "source": [
    "show_performance(train_forest('sweet', 'dark'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Preprocessing training set...\n",
      "Preprocessing validation set...\n",
      "Trimming input...\n",
      "Training LSTM...\n",
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "embedding_5 (Embedding)      (None, 150, 64)           320064    \n",
      "_________________________________________________________________\n",
      "dropout_13 (Dropout)         (None, 150, 64)           0         \n",
      "_________________________________________________________________\n",
      "conv1d_5 (Conv1D)            (None, 150, 64)           12352     \n",
      "_________________________________________________________________\n",
      "max_pooling1d_5 (MaxPooling1 (None, 75, 64)            0         \n",
      "_________________________________________________________________\n",
      "dropout_14 (Dropout)         (None, 75, 64)            0         \n",
      "_________________________________________________________________\n",
      "lstm_5 (LSTM)                (None, 50)                23000     \n",
      "_________________________________________________________________\n",
      "dropout_15 (Dropout)         (None, 50)                0         \n",
      "_________________________________________________________________\n",
      "dense_5 (Dense)              (None, 1)                 51        \n",
      "=================================================================\n",
      "Total params: 355,467\n",
      "Trainable params: 355,467\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n",
      "None\n",
      "Train on 7200 samples, validate on 1801 samples\n",
      "Epoch 1/5\n",
      "7200/7200 [==============================] - 10s 1ms/step - loss: 0.6590 - acc: 0.5960 - val_loss: 0.6033 - val_acc: 0.6780\n",
      "Epoch 2/5\n",
      "7200/7200 [==============================] - 9s 1ms/step - loss: 0.5216 - acc: 0.7535 - val_loss: 0.5627 - val_acc: 0.7290\n",
      "Epoch 3/5\n",
      "7200/7200 [==============================] - 10s 1ms/step - loss: 0.4570 - acc: 0.8057 - val_loss: 0.5637 - val_acc: 0.7435\n",
      "Epoch 4/5\n",
      "7200/7200 [==============================] - 9s 1ms/step - loss: 0.4110 - acc: 0.8319 - val_loss: 0.5752 - val_acc: 0.7235\n",
      "Epoch 5/5\n",
      "7200/7200 [==============================] - 10s 1ms/step - loss: 0.3750 - acc: 0.8465 - val_loss: 0.6041 - val_acc: 0.7324\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "(<keras.engine.sequential.Sequential at 0x1a33858b70>,\n",
       " array([[ 154,   57,    6, ...,    6,  257,   37],\n",
       "        [  67,    2, 3514, ...,   59,   14, 1255],\n",
       "        [  17,    2,  563, ...,   50,  283,   50],\n",
       "        ...,\n",
       "        [   0,    0,    0, ...,    6,  279,    0],\n",
       "        [  25, 3795,   49, ...,   41,   11,   73],\n",
       "        [ 798,  106,    6, ...,   66,  747,  798]], dtype=int32),\n",
       " array([0, 1, 0, ..., 0, 1, 0]),\n",
       " array([[  61,   11, 3252, ...,  200,   92, 2211],\n",
       "        [   3,  110,  392, ...,  392,  110,  392],\n",
       "        [  74,  762,    1, ...,    4,  131,    1],\n",
       "        ...,\n",
       "        [   1,   57, 2539, ...,    0,    0,    0],\n",
       "        [ 290,    6,  138, ...,   92,    1, 3856],\n",
       "        [ 476,   22,  257, ..., 1110,   22,  251]], dtype=int32),\n",
       " array([0, 0, 1, ..., 1, 0, 0]))"
      ]
     },
     "execution_count": 19,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train_lstm('sweet', 'dark')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Part1.3: final model for emotion recognition"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We built four pipelines for four dimensions of emotion: happy-sad, relax-energetic, cool-oldies, sweet-dark."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def train_forest(X, y):\n",
    "    vectorizer = HashingVectorizer(n_features=1000)\n",
    "  \n",
    "    print('Preprocessing training set...')\n",
    "    X_train_forest = vectorizer.fit_transform(X)\n",
    "    y_train_forest = y\n",
    "    print('Training random forest...')\n",
    "    forest = RandomForestClassifier(\n",
    "        n_estimators=300, max_features=30, max_depth=None, min_samples_split=2)\n",
    "    forest.fit(X_train_forest, y_train_forest)\n",
    "    print('Finished.')\n",
    "    return forest, vectorizer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "def save_forest_pipeline(res, tag1, tag2):\n",
    "    pipeline = Pipeline([('vectorizer', res[1]), ('forest', res[0])])\n",
    "    dump(pipeline, tag1+'-'+tag2+'pipeline.joblib')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "def load_forest_pipeline(tag1, tag2):\n",
    "    return load(tag1+'-'+tag2+'pipeline.joblib')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "def create_pipeline(tag1, tag2):\n",
    "    X, y = create_train(tag1, tag2)\n",
    "    res = train_forest(X, y)\n",
    "    save_forest_pipeline(res, tag1, tag2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Collecting tids...\n",
      "Collecting artists and titles...\n",
      "Collecting lyrics...\n",
      "8508 tracks collected for tag happy, 9078 tracks collected for tag sad\n",
      "Filtering out empty lyrics...\n",
      "Filtering out non-English lyrics...\n",
      "7721 nonempty lyrics for tag happy, 8302 nonempty lyrics for tag sad\n",
      "Creating predictor set and target set...\n",
      "Preprocessing training set...\n",
      "Training random forest...\n",
      "Finished.\n"
     ]
    }
   ],
   "source": [
    "create_pipeline('happy', 'sad')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Collecting tids...\n",
      "Collecting artists and titles...\n",
      "Collecting lyrics...\n",
      "6287 tracks collected for tag relax, 4521 tracks collected for tag Energetic\n",
      "Filtering out empty lyrics...\n",
      "Filtering out non-English lyrics...\n",
      "5214 nonempty lyrics for tag relax, 4072 nonempty lyrics for tag Energetic\n",
      "Creating predictor set and target set...\n",
      "Preprocessing training set...\n",
      "Training random forest...\n",
      "Finished.\n"
     ]
    }
   ],
   "source": [
    "create_pipeline('relax', 'Energetic')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Collecting tids...\n",
      "Collecting artists and titles...\n",
      "Collecting lyrics...\n",
      "12457 tracks collected for tag cool, 12769 tracks collected for tag oldies\n",
      "Filtering out empty lyrics...\n",
      "Filtering out non-English lyrics...\n",
      "11062 nonempty lyrics for tag cool, 11969 nonempty lyrics for tag oldies\n",
      "Creating predictor set and target set...\n",
      "Preprocessing training set...\n",
      "Training random forest...\n",
      "Finished.\n"
     ]
    }
   ],
   "source": [
    "create_pipeline('cool', 'oldies')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Collecting tids...\n",
      "Collecting artists and titles...\n",
      "Collecting lyrics...\n",
      "5169 tracks collected for tag sweet, 4768 tracks collected for tag dark\n",
      "Filtering out empty lyrics...\n",
      "Filtering out non-English lyrics...\n",
      "4774 nonempty lyrics for tag sweet, 4225 nonempty lyrics for tag dark\n",
      "Creating predictor set and target set...\n",
      "Preprocessing training set...\n",
      "Training random forest...\n",
      "Finished.\n"
     ]
    }
   ],
   "source": [
    "create_pipeline('sweet', 'dark')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Part1.4: profiling the emotion of tracks"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We profiled the tracks in the Million Playlist Dataset using the four piplines."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "TRACKURI_PATH = 'databases/track_uri.db'\n",
    "PROFILES_PATH = 'databases/profiles.db'\n",
    "\n",
    "conn_trackuri = sqlite3.connect(TRACKURI_PATH)\n",
    "conn_profiles = sqlite3.connect(PROFILES_PATH)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "conn_profiles.execute(\"\"\"CREATE TABLE profiles (\n",
    "                         track_uri TEXT,\n",
    "                         happy_sad REAL,\n",
    "                         relax_energetic REAL,\n",
    "                         cool_oldies REAL,\n",
    "                         sweet_dark REAL)\"\"\")\n",
    "conn_profiles.execute(\"\"\"CREATE INDEX profile_track_uris ON profiles (track_uri)\"\"\")\n",
    "conn_profiles.commit()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\"\n",
    "Profile the emotion of a song:\n",
    "[\n",
    "1: happy, -1:sad;\n",
    "1: relax, -1:energetic;\n",
    "1: cool,  -1:oldies;\n",
    "1: sweet, -1:dark;\n",
    "]\n",
    "\"\"\"\n",
    "def make_profile(lyrics, pipelinelist):\n",
    "    profiles = np.zeros(shape=(len(lyrics), len(pipelinelist)))\n",
    "    for i, pipeline in enumerate(pipelinelist):\n",
    "        profiles[:, i] = (pipeline.predict_proba(lyrics)[:, 1] - 0.5) * 2.0\n",
    "    return profiles"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def build_profiles(urilist, conn_trackuri, conn_wiki, \n",
    "                   conn_profiles):\n",
    "    AandTs = get_artistsandtitles_byuri(urilist, conn_trackuri)\n",
    "    lyrics = get_lyrics(AandTs, conn_wiki)\n",
    "    urilist_, lyrics_ = get_nonempty(urilist, lyrics)\n",
    "    urilist_, lyrics_ = get_english(urilist_, lyrics_)\n",
    "    assert len(urilist_) == len(set(urilist_))\n",
    "    profiles = make_profile(lyrics_, \n",
    "                            [pipeline1, pipeline2, pipeline3, pipeline4])\n",
    "    insertvals = [(uri, profile[0], profile[1], profile[2], profile[3]) \n",
    "                  for uri, profile in zip(urilist_, profiles)]\n",
    "    conn_profiles.executemany(\"\"\"INSERT INTO profiles (track_uri, happy_sad, relax_energetic, cool_oldies, sweet_dark)\n",
    "                             VALUES (?, ?, ?, ?, ?)\"\"\", insertvals)\n",
    "    conn_profiles.commit()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "batch_size = 1000\n",
    "batch_numb = len(all_uris) // batch_size\n",
    "for i in range(batch_numb+1):\n",
    "    batch = all_uris[batch_size*i: batch_size*(i+1)]\n",
    "    build_profiles(batch, conn_trackuri, conn_wiki, conn_profiles)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Part1.5: profiling the audio features and genre features of tracks"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The profiles for audio features are obtained from Million Songs Dataset and the genre features are extracted from the audio featrues using a random forest model. They too were inserted into a relational database for latter use."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "AUDIOPROFILES_PATH = 'databases/audio_profiles.db'\n",
    "GENREPROFILES_PATH = 'databases/genre_profiles.db'\n",
    "\n",
    "conn_audioprofiles = sqlite3.connect(AUDIOPROFILES_PATH)\n",
    "conn_genreprofiles = sqlite3.connect(GENREPROFILES_PATH)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "audio_df = pd.read_csv('MPD_audio_full.csv')\n",
    "# later I'll hard-code the table name 'profile' into the code...\n",
    "# self-reminder: make this table name a convention.\n",
    "audio_df.to_sql('profiles', con=conn_audioprofiles, index=False)\n",
    "# build index to accelerate querying\n",
    "conn_audioprofiles.execute(\"\"\"CREATE INDEX profile_track_uris ON profiles (track_uri)\"\"\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "genre_df = pd.read_csv('MPD_genre.csv')\n",
    "genre_df.to_sql('profiles', con=conn_genreprofiles, index=False)\n",
    "conn_genreprofiles.execute(\"\"\"CREATE INDEX profile_track_uris ON profiles (track_uri)\"\"\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Part2: Recommendiations based on profiles"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The recommendations were generated by finding tracks that has most similar profiles as the input (seeding) tracks. The similarity is defined by cosine similarity and the we achived efficient retrieval of similarity neighbors by putting all our tracks into a KD-tree (It can be proved that when the profiles are normalized, the tracks that have smallest euclidean distances are the tracks that have the highest cosine similarity). To address the problem that the tracks in an input playlist may not be homogeneous, we first perform a clustering on the input tracks and then give recommendations based on each clustering center."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 121,
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\"functions to query database for titles and profiles\"\"\"\n",
    "def get_titles(seed_urilist):\n",
    "    titles   = [get_title_byuri(uri) for uri in seed_urilist]\n",
    "    return [t for t in titles if t is not None]\n",
    "def get_profiles(seed_urilist, conn_profiles):\n",
    "    profiles = [get_profile_byuri(uri, conn_profiles) for uri in seed_urilist]\n",
    "    return np.array([p for p in profiles if p is not None])\n",
    "def get_title_byuri(uri):\n",
    "    sql  = \"SELECT track_name FROM track_uri WHERE track_uri='{}'\".format(uri)\n",
    "    res  = conn_trackuri.execute(sql)\n",
    "    try:\n",
    "        return res.fetchall()[0][0]\n",
    "    except:\n",
    "        return None\n",
    "def get_profile_byuri(uri, conn_profiles):\n",
    "    sql  = \"SELECT * FROM profiles WHERE track_uri='{}'\".format(uri)\n",
    "    res  = conn_profiles.execute(sql)\n",
    "    try:\n",
    "        return res.fetchall()[0][1:]\n",
    "    except:\n",
    "        return None\n",
    "def get_all_profiles(conn_profiles, dropnull=False):\n",
    "    sql  = \"\"\"SELECT * from profiles\"\"\"\n",
    "    res  = conn_profiles.execute(sql)\n",
    "    data = res.fetchall()\n",
    "    if dropnull:\n",
    "        for p in data:\n",
    "            if None in p: data.remove(p)\n",
    "    return data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 122,
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\"set up kdtree for a certain kind of profiles\"\"\"\n",
    "def build_kdtree(conn_profiles):\n",
    "    print('Reading profiles from sqlite...')\n",
    "    all_profiles = get_all_profiles(conn_profiles, dropnull=True)\n",
    "    print('Creating dictionary got URI ...')\n",
    "    dict_ind2uri = {i: t[0] for i, t in enumerate(all_profiles)}\n",
    "    dict_uri2ind = {v: k    for k, v in dict_ind2uri.items()   }\n",
    "    print('Building KD-tree...')\n",
    "    profile_values = np.array([t[1:] for t in all_profiles])\n",
    "    # profiles are normalized so that their euclidian neighbors are also their cosine-similarity neighbors\n",
    "    profile_values = normalize(profile_values) \n",
    "    kdtree = KDTree(profile_values, leaf_size=2)\n",
    "    return kdtree, dict_ind2uri, dict_uri2ind"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 123,
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\"make recommendation based on single seed profile\"\"\"\n",
    "def get_recm_byprofile(seed_profile, kdtree, dict_ind2uri, exclude_titles, num_recm):\n",
    "    search_ratio = 2\n",
    "    while True:\n",
    "        _,    ind = kdtree.query([seed_profile], k=num_recm*search_ratio)\n",
    "        recm_list = []\n",
    "        for recm_ind in ind[0]:\n",
    "            recm_uri   = dict_ind2uri[recm_ind]\n",
    "            recm_title = get_title_byuri(recm_uri)\n",
    "            if recm_title in exclude_titles: continue\n",
    "            recm_list.append(recm_uri)\n",
    "            if len(recm_list) == num_recm:   return recm_list\n",
    "        if len(recm_list) < num_recm: search_ratio *= 2\n",
    "    return recm_list\n",
    "\"\"\"make recommendation based on multiple seed profiles\"\"\"\n",
    "def create_recommendations(seed_profiles, kdtree, dict_ind2uri, exclude_titles, num_recm=500, num_cntr=None):\n",
    "    if len(seed_profiles) == 0: return []\n",
    "    num_seed = len(seed_profiles)\n",
    "    if num_cntr is None: num_cntr = int(np.sqrt(num_seed))\n",
    "    kmeans = KMeans(n_clusters=num_cntr).fit(seed_profiles)\n",
    "    num_recm_per_cntr = int(num_recm/num_cntr)+1\n",
    "    lists = []\n",
    "    for c in kmeans.cluster_centers_:\n",
    "        lists.append( get_recm_byprofile(c, kdtree, dict_ind2uri, exclude_titles, num_recm_per_cntr) )\n",
    "    recm_list = [None] * num_cntr * num_recm_per_cntr\n",
    "    for i in range(num_cntr):\n",
    "        recm_list[i::num_cntr] = lists[i]\n",
    "    return recm_list[:num_recm]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 124,
   "metadata": {},
   "outputs": [],
   "source": [
    "def generate_val_Y(val_X, conn_profiles, kdtree, dict_ind2uri, num_recm_per_list, verbose=True):\n",
    "    val_Y = []\n",
    "    total = len(val_X); every = total // 100\n",
    "    for i, seedlist in enumerate(val_X):\n",
    "        titles   = get_titles(seedlist)\n",
    "        profiles = get_profiles(seedlist, conn_profiles)\n",
    "        val_Y.append( create_recommendations(profiles, kdtree, dict_ind2uri, \n",
    "                                             exclude_titles=titles, num_recm=num_recm_per_list) )\n",
    "        if verbose and i%every == 0: print('{0:3d} / 100 finished'.format(i//every))\n",
    "    return val_Y"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Reading profiles from sqlite...\n",
      "Creating dictionary got URI ...\n",
      "Building KD-tree...\n"
     ]
    }
   ],
   "source": [
    "genrekdtree, genredict_ind2uri, genredict_uri2ind = build_kdtree(conn_genreprofiles)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "genreval_Y = generate_val_Y(val_X, conn_genreprofiles, genrekdtree, genredict_ind2uri, 500, verbose=True)\n",
    "with open('val_Y_genre.json', 'w') as f:\n",
    "    json.dump(genreval_Y, f)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 63,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Reading profiles from sqlite...\n",
      "Creating dictionary got URI ...\n",
      "Building KD-tree...\n"
     ]
    }
   ],
   "source": [
    "audiokdtree, audiodict_ind2uri, audiodict_uri2ind = build_kdtree(conn_audioprofiles)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "audioval_Y = generate_val_Y(val_X, conn_audioprofiles, audiokdtree, audiodict_ind2uri, 500, verbose=True)\n",
    "with open('val_Y_audio.json', 'w') as f:\n",
    "    json.dump(audioval_Y, f)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 66,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Reading profiles from sqlite...\n",
      "Creating dictionary got URI ...\n",
      "Building KD-tree...\n"
     ]
    }
   ],
   "source": [
    "lyrickdtree, lyricdict_ind2uri, lyricdict_uri2ind = build_kdtree(conn_lyricprofiles)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "lyricval_Y = generate_val_Y(val_X, conn_lyricprofiles, lyrickdtree, lyricdict_ind2uri, 500, verbose=True)\n",
    "with open('val_Y_lyric.json', 'w') as f:\n",
    "    json.dump(lyricval_Y, f)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def scoring(val_Y, method='linear'):\n",
    "    scores = []\n",
    "    length = len(val_Y[0])\n",
    "    if method == 'linear':\n",
    "        for l in val_Y:\n",
    "            score_dict = {}\n",
    "            for i, t in enumerate(l):\n",
    "                score_dict[t] = 1-i/length\n",
    "            scores.append(score_dict)\n",
    "    elif method == 'exp':\n",
    "        for l in val_Y:\n",
    "            score_dict = {}\n",
    "            for i, t in enumerate(l):\n",
    "                score_dict[t] = np.exp(-4*i/length)\n",
    "            scores.append(score_dict)\n",
    "    else: raise ValueError('method not found')\n",
    "    return scores"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "genreval_Y_score = scoring(genreval_Y)\n",
    "audioval_Y_score = scoring(audioval_Y)\n",
    "lyricval_Y_score = scoring(lyricval_Y)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
