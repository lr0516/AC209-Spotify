{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Collaborative Filtering "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "It is intuitive that people who listen to the same songs have similar tastes in music, and this is the basic assumption of collaborative filtering. Interactions between a bunch of users and a bunch of items can be represented by a utility matrix with a lot of blanks (since some of the interactions are absent). Collaborative filtering techniques fill in the blanks of the utility matrix by giving predictions based on existing interactions. We build up the collaborative filtering models based on Million Playlist Dataset, assuming that users with similar past behaviors are likely to listen to the same songs in the future.\n",
    "\n",
    "To build up the collborative filtering models, we use matrix factorization technique. The idea behind matrix factorization technique is that users actually respond to some very specific features of items and these features could be captured by decomposing the $n*m$ utility matrix into two matrices whose dimensions are $$n*d$$ and $$d*m$$. (Here d is the number of features characterized by the specific decomposition.) When people listen to music, they may be attracted by some very specific characteristics such as artist, genre and emotions passed by the music. So matrix factorization technique is suitable for music recommendation system.\n",
    "\n",
    "We use the [Spotlight library](https://maciejkula.github.io/spotlight/index.html) which is based on PyTorch to build up matrix factorization models to get prediction utility matrix for all the users on all the items included in the training data set. We use the implicit feedback models as users do not give explicit ratings for the songs in the playlists. Only presence and absence of events can be captured from the Million Playlist Dataset.\n",
    "\n",
    "Now that utility matrix is determined, for each user in the test dataset, we predict his/her ratings on the tracks by calculating the weighted sum of training dataset users’ ratings on the tracks. The weight of users in the training dataset is defined by cosine similarity between users in the training dataset and the specific tested user.\n",
    "\n",
    "We have three collaborative filtering models which are:\n",
    "    - Baseline Model\n",
    "    - Filtered-Data Model\n",
    "    - Meta-Playlist Model\n",
    " These three models differ the most in their training datasets."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## I. Baseline Model"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We build up the baseline model by simply feeding original (playlist, song) pairs from 50,000 playlists to the Implicit Feedback Factorization model. 5% playlists from the original MDP dataset are used to train the model, which include 50,000 playlists, 450,000 songs and 3,320,000 (playlist, song) pairs. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# To get 5% playlists data\n",
    "import numpy as np\n",
    "import json\n",
    "import os\n",
    "\n",
    "data_path = os.getcwd()+'/millionplaylist/data/'\n",
    "playlist_fn = os.listdir(data_path)\n",
    "\n",
    "CF_list = []\n",
    "\n",
    "for fn_index in range(50): #range(len(playlist_fn)):\n",
    "    with open(data_path+playlist_fn[fn_index]) as f:\n",
    "        data = json.load(f)\n",
    "\n",
    "    playlists = data['playlists']\n",
    "\n",
    "    for playlist in playlists:\n",
    "        pid = playlist['pid']\n",
    "        for song in playlist['tracks']:\n",
    "            track_uri = song['track_uri'].split(':')[2]\n",
    "            CF_list.append([pid,track_uri])\n",
    "\n",
    "import json\n",
    "with open(\"CF_lists_50000.json\", \"w\") as f:\n",
    "    data_json = json.dump(CF_list,f)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# To import the necessary libraries\n",
    "from spotlight.interactions import Interactions\n",
    "from spotlight.evaluation import rmse_score\n",
    "from spotlight.factorization.implicit import ImplicitFactorizationModel\n",
    "\n",
    "# Load data\n",
    "\n",
    "with open('CF_lists_50000.json', 'r') as f:\n",
    "    data = json.load(f)\n",
    "\n",
    "user = [int(u) for u, i in data]\n",
    "item = [i for u,i  in data]\n",
    "\n",
    "# Assign IDs to the playlists\n",
    "\n",
    "count_u = {}\n",
    "for u in user:\n",
    "    if u not in count_u.keys():\n",
    "        count_u[u] = 1\n",
    "    else:\n",
    "        count_u[u] = count_u[u] + 1\n",
    "\n",
    "user_id = {}\n",
    "id = 0\n",
    "for u in count_u.keys():\n",
    "    user_id[u] = id\n",
    "    id = id+1\n",
    "\n",
    "user_processed = []\n",
    "for u in user:\n",
    "    user_processed.append(user_id[u])\n",
    "\n",
    "# Assign IDs to the items\n",
    "\n",
    "count = {}\n",
    "for i in item:\n",
    "    if i not in count.keys():\n",
    "        count[i] = 1\n",
    "    else:\n",
    "        count[i] = count[i] + 1\n",
    "\n",
    "item_id = {}\n",
    "id = 0\n",
    "for i in count.keys():\n",
    "    item_id[i] = id\n",
    "    id = id+1\n",
    "\n",
    "\n",
    "len(item_id.keys())\n",
    "\n",
    "item_processed = []\n",
    "for i in item:\n",
    "    item_processed.append(item_id[i])\n",
    "\n",
    "# Build up relationships between track IDs and item IDs\n",
    "id_to_track = [0] * len(item_id.keys())\n",
    "for item in item_id.keys():\n",
    "    id_to_track[item_id[item]] = item\n",
    "\n",
    "# Build up dataset which can be fed into the Spotlight model\n",
    "\n",
    "rating = np.ones(len(item))\n",
    "\n",
    "data = Interactions(np.array(user_processed), np.array(item_processed), rating)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Train the model\n",
    "\n",
    "model = ImplicitFactorizationModel(n_iter = 1)\n",
    "model.fit(data, verbose = 1)\n",
    "torch.save(model, 'baseline_model')\n",
    "model = torch.load('baseline_model')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Generate 500 songs with the highest similarities to input playlists\n",
    "\n",
    "with open('Val_X.json', 'r') as f:\n",
    "    validation = json.load(f)\n",
    "\n",
    "def top_500_dic(inp):\n",
    "    dic = {}\n",
    "    score = np.array([0] * len(model.predict(1)))\n",
    "    sum_w = 0\n",
    "    users = np.array(range(50000))\n",
    "    random.shuffle(users)\n",
    "    users = users[0:99]\n",
    "    for i in users:\n",
    "        s = model.predict(i, np.array(inp))\n",
    "        w = sum(s) / (np.linalg.norm(s) * np.sqrt(len(inp)))\n",
    "        sum_w = sum_w + w\n",
    "        score = w * model.predict(i) + score\n",
    "    score = score / sum_w\n",
    "    for index in np.argsort(score)[-1:-501:-1]:\n",
    "        dic[id_to_track[index]] = score[index]\n",
    "    return dic\n",
    "\n",
    "\n",
    "rec = []\n",
    "for i in range(len(validation)):\n",
    "    each_input = validation[i]\n",
    "    input_id = []\n",
    "    for j in range(len(each_input)):\n",
    "        it = each_input[j]\n",
    "        if it in item_id.keys():\n",
    "            input_id.append(item_id[it]) \n",
    "    if(len(input_id) > 1):\n",
    "        rec.append(top_500_dic(input_id))\n",
    "    else:\n",
    "        rec.append(popular_dic)\n",
    "    print('Rec completed', i)\n",
    "\n",
    "# Give back the recommended songs\n",
    "with open('Recommend_songs_baseline_CFmodel.json', 'w') as f:\n",
    "    json.dump(rec, f)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "A weakness of the baseline model is that some of the songs in the training datasets are included in only one playlist and some of the playlists in the training datasets are too short to describe the similarities between songs. Another problem is that since it takes too long to train the Implicit Feedback Factorization models, only 5% of the MDP data were used. To overcome these problems, we further build up Filtered-Data Model and Meta-Playlist Model by training the Implicit Feedback Factorization model on processed data."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## II. Filtered-Data Model"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "For Filtered-Data Model, we filter the MDP data by only maintaining songs which appear in 10 or more playlists, which results in 190,000 playlists with more than 50 songs. Then we sample from the 190,000 playlists to get 50,000 playlists as the training dataset, which includes 50,000 playlists, 320,000 songs and 22,900,000 (playlist, song) pairs. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Get Filtered Data\n",
    "\n",
    "# Remove Duplicate Songs\n",
    "song_df = pd.read_csv('song_df_orig.csv')\n",
    "\n",
    "pid_lists = song_df['pid'].values\n",
    "pid_lists = [list(set(pid_list.split('[')[1].split(']')[0].split(', '))) for pid_list in pid_lists]\n",
    "num_pid = [len(pid_list) for pid_list in pid_lists]\n",
    "\n",
    "song_df['num_pid'] = num_pid\n",
    "song_df['pid'] = pid_lists\n",
    "\n",
    "song_df.columns = ['track_uri']+list(song_df.columns)[1:]\n",
    "\n",
    "# Filter out songs that appear in fewer than 10 playlists\n",
    "song_df_filtered = song_df.loc[song_df['num_pid']>=10]\n",
    "# We are left with 369,199 songs\n",
    "\n",
    "# Decide Playlist Length Criterion\n",
    "df_filtered = song_df_filtered[['pid']]\n",
    "df_filtered.index = song_df_filtered['track_uri']\n",
    "dict_filtered = df_filtered.to_dict()['pid']\n",
    "\n",
    "CF_filtered = []\n",
    "for uri in dict_filtered:\n",
    "    pids = dict_filtered[uri].split('\\'')\n",
    "    for pid in pids:\n",
    "        try: CF_filtered.append([int(pid),uri])\n",
    "        except ValueError: pass\n",
    "        \n",
    "user = [u for u,i in CF_filtered]\n",
    "user_count = Counter(user)\n",
    "length_count = Counter(np.array(sorted(user_count.items()))[:,1])\n",
    "length_count = np.array(sorted(length_count.items()))\n",
    "print(sum(length_count[99:,1]))\n",
    "# We are left with 194,052 playlists\n",
    "\n",
    "# Filter Playlists\n",
    "with open(\"val_pid.json\", \"r\") as f:\n",
    "    filter_pid = json.load(f)\n",
    "\n",
    "user_count = np.array(sorted(user_count.items()))\n",
    "for pid,count in user_count:\n",
    "    if count < 100: filter_pid.append(pid)\n",
    "\n",
    "full_pid = list(set(np.array(CF_filtered)[:,0]))\n",
    "full_pid = [int(i) for i in full_pid]\n",
    "keep_pid = list(set(full_pid) - set(filter_pid))\n",
    "keep_dict = {i:1 for i in keep_pid}\n",
    "\n",
    "import random\n",
    "keep_pid_2 = random.sample(keep_pid,50000)\n",
    "keep_dict = {i:1 for i in keep_pid_2}\n",
    "\n",
    "CF_filtered2 = []\n",
    "for uri in dict_filtered:\n",
    "    for pid in dict_filtered[uri].split('\\''):\n",
    "        try: \n",
    "            current_pid = int(pid)\n",
    "            try:\n",
    "                if keep_dict[current_pid]: CF_filtered2.append([current_pid,uri]) \n",
    "            except KeyError: pass\n",
    "        except ValueError: pass\n",
    "\n",
    "# Save to json\n",
    "with open(\"CF_lists_filtered.json\", \"w\") as f:\n",
    "    data_json = json.dump(CF_filtered2,f)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# To import the necessary libraries\n",
    "\n",
    "import numpy as np\n",
    "import json\n",
    "from spotlight.interactions import Interactions\n",
    "from spotlight.evaluation import rmse_score\n",
    "from spotlight.factorization.implicit import ImplicitFactorizationModel\n",
    "\n",
    "# Load data\n",
    "\n",
    "with open('CF_lists_filtered.json', 'r') as f:\n",
    "    data = json.load(f)\n",
    "\n",
    "user = [int(u) for u, i in data]\n",
    "item = [i for u,i  in data]\n",
    "\n",
    "# Assign IDs to the playlists\n",
    "\n",
    "count_u = {}\n",
    "for u in user:\n",
    "    if u not in count_u.keys():\n",
    "        count_u[u] = 1\n",
    "    else:\n",
    "        count_u[u] = count_u[u] + 1\n",
    "\n",
    "user_id = {}\n",
    "id = 0\n",
    "for u in count_u.keys():\n",
    "    user_id[u] = id\n",
    "    id = id+1\n",
    "\n",
    "user_processed = []\n",
    "for u in user:\n",
    "    user_processed.append(user_id[u])\n",
    "\n",
    "# Assign IDs to the items\n",
    "\n",
    "count = {}\n",
    "for i in item:\n",
    "    if i not in count.keys():\n",
    "        count[i] = 1\n",
    "    else:\n",
    "        count[i] = count[i] + 1\n",
    "\n",
    "item_id = {}\n",
    "id = 0\n",
    "for i in count.keys():\n",
    "    item_id[i] = id\n",
    "    id = id+1\n",
    "\n",
    "\n",
    "len(item_id.keys())\n",
    "\n",
    "item_processed = []\n",
    "for i in item:\n",
    "    item_processed.append(item_id[i])\n",
    "\n",
    "# Build up relationships between track IDs and item IDs\n",
    "id_to_track = [0] * len(item_id.keys())\n",
    "for item in item_id.keys():\n",
    "    id_to_track[item_id[item]] = item\n",
    "\n",
    "# Build up dataset which can be fed into the Spotlight model\n",
    "\n",
    "rating = np.ones(len(item))\n",
    "\n",
    "data = Interactions(np.array(user_processed), np.array(item_processed), rating)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Train the model\n",
    "\n",
    "model = ImplicitFactorizationModel(n_iter = 1)\n",
    "model.fit(data, verbose = 1)\n",
    "torch.save(model, 'advanced_model')\n",
    "model = torch.load('advanced_model.dms')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Generate 500 songs with the highest similarities to input playlists\n",
    "\n",
    "with open('Val_X.json', 'r') as f:\n",
    "    validation = json.load(f)\n",
    "\n",
    "def top_500_dic(inp):\n",
    "    dic = {}\n",
    "    score = np.array([0] * len(model.predict(1)))\n",
    "    sum_w = 0\n",
    "    users = np.array(range(50000))\n",
    "    random.shuffle(users)\n",
    "    users = users[0:99]\n",
    "    for i in users:\n",
    "        s = model.predict(i, np.array(inp))\n",
    "        w = sum(s) / (np.linalg.norm(s) * np.sqrt(len(inp)))\n",
    "        sum_w = sum_w + w\n",
    "        score = w * model.predict(i) + score\n",
    "    score = score / sum_w\n",
    "    for index in np.argsort(score)[-1:-501:-1]:\n",
    "        dic[id_to_track[index]] = score[index]\n",
    "    return dic\n",
    "\n",
    "rec = []\n",
    "for i in range(len(validation)):\n",
    "    each_input = validation[i]\n",
    "    input_id = []\n",
    "    for j in range(len(each_input)):\n",
    "        it = each_input[j]\n",
    "        if it in item_id.keys():\n",
    "            input_id.append(item_id[it]) \n",
    "    if(len(input_id) > 1):\n",
    "        rec.append(top_500_dic(input_id))\n",
    "    else:\n",
    "        rec.append(popular_dic)\n",
    "    print('Rec completed', i)\n",
    "\n",
    "# Give back the recommended songs\n",
    "with open('Recommend_songs_advanced_CFmodel.json', 'w') as f:\n",
    "    json.dump(rec, f)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## III. Meta-Playlist Model"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "To use as many data as possible, we created these ‘meta-playlist’. We observed that the\n",
    "MPD dataset contains many sets of playlists with shared titles, the top five of which being ‘country’,\n",
    "‘chill’, ‘rap’, ‘workout’, and ‘oldies’. We combined each set of playlists with the same title into a\n",
    "meta-playlist. Each meta-playlist contains all the songs from the sub-playlists and keeps track of the\n",
    "number of times each song appears in all the sub-playlists.\n",
    "Then we fitted a collaborative filtering model on the meta-playlists. This method contains fewer rows\n",
    "and more columns compared to the previous model described."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Currently, we selected the top 100 common titles to be the titles of our 100 meta-playlists.\n",
    "This includes 272,584 playlists from MPD in total and 939,760 distinct songs, which give us 4,620,153\n",
    "(playlist, song, rating) pairs. We treated each meta-playlist as one user, whose music preference is clearly\n",
    "stated in the title. The number of appearance of each song was treated as the song’s rating by the user.\n",
    "The final model would give a score to every song included in the training dataset for every playlist\n",
    "in the test dataset. For every playlist in the test dataset, we could generate the songs to which the model\n",
    "thinks that the playlist will give rather high scores."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We built up an implicit feedback model and use matrix factorization techniques for this\n",
    "problem . We fed the model with (playlist, song, rating) interactions. 80% of the data were used for\n",
    "training, the rest was used for validation. 10 epoches were run to give an idea on this model."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Get Data by Top Playlist Names\n",
    "import re\n",
    "from collections import Counter\n",
    "\n",
    "def normalize_name(name):\n",
    "    name = name.lower()\n",
    "    name = re.sub(r\"[.,\\/#!$%\\^\\*;:{}=\\_`~()@]\", ' ', name)\n",
    "    name = re.sub(r'\\s+', ' ', name).strip()\n",
    "    return name\n",
    "\n",
    "data_path = os.path.join(os.getcwd(),'millionplaylist','data')\n",
    "filenames = os.listdir(data_path)\n",
    "\n",
    "names = []\n",
    "for filename in sorted(filenames):\n",
    "    with open(os.path.join(data_path,filename)) as f:\n",
    "        file_data = json.load(f)\n",
    "\n",
    "    for playlist in file_data['playlists']:\n",
    "        names.append(normalize_name(playlist['name']))\n",
    "\n",
    "name_count = Counter(names).most_common(100)\n",
    "print(name_count)\n",
    "\n",
    "top_names = [pair[0] for pair in name_count]\n",
    "CF_dict = {name:{} for name in top_names}\n",
    "\n",
    "progress_count = 0\n",
    "for filename in filenames:\n",
    "    with open(os.path.join(data_path,filename)) as f:\n",
    "        file_data = json.load(f)\n",
    "    \n",
    "    for playlist in file_data['playlists']:\n",
    "        progress_count += 1\n",
    "        print(progress_count)\n",
    "        name_norm = normalize_name(playlist['name'])\n",
    "        if name_norm in top_names:\n",
    "            for song in playlist['tracks']:\n",
    "                try: CF_dict[name_norm][song['track_uri'].split(':')[2]] += 1\n",
    "                except KeyError: CF_dict[name_norm][song['track_uri'].split(':')[2]] = 1\n",
    "\n",
    "CF_list = []\n",
    "for name in CF_dict:\n",
    "    songs = CF_dict[name]\n",
    "    for uri in songs:\n",
    "        CF_list.append([name,uri,songs[uri]])\n",
    "\n",
    "with open(\"CF_matrix_condensed.json\", \"w\") as f:\n",
    "    data_json = json.dump(CF_list,f)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "\n",
    "from spotlight.interactions import Interactions\n",
    "from spotlight.cross_validation import random_train_test_split\n",
    "from spotlight.datasets.movielens import get_movielens_dataset\n",
    "from spotlight.evaluation import rmse_score\n",
    "from spotlight.evaluation import mrr_score\n",
    "from spotlight.evaluation import precision_recall_score\n",
    "from spotlight.factorization.explicit import ExplicitFactorizationModel\n",
    "from spotlight.factorization.implicit import ImplicitFactorizationModel\n",
    "\n",
    "from enum import Enum"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load data\n",
    "import json\n",
    "with open('CF_matrix_condensed.json', 'r') as f:\n",
    "    data = json.load(f)\n",
    "    \n",
    "user = [u for u, i, r in data]\n",
    "item = [i for u, i, r in data]\n",
    "rating = [r for u, i, r in data]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create an Enum object to index the user names\n",
    "User_id = Enum(value = 'User_id', names = list(set(user)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Replace the user name strings with integer indices specified in the Enum object\n",
    "for i in range(len(user)):\n",
    "    u = user[i]\n",
    "    user[i] = User_id[u].value"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Replace the item id strings with integer indices specified in the Enum object\n",
    "a = list(set(item))\n",
    "\n",
    "item_id = {}\n",
    "id = 1\n",
    "for i in range(len(a)):\n",
    "    item_id[a[i]] = id\n",
    "    id = id+1\n",
    "\n",
    "for i in range(len(item)):\n",
    "    it = item[i]\n",
    "    item[i] = item_id[it]    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Train the full model with 10 epochs\n",
    "data = Interactions(user_ids=np.array(user), item_ids=np.array(item), ratings=np.array(rating))\n",
    "train, test = random_train_test_split(data)\n",
    "model_full = ImplicitFactorizationModel()\n",
    "model_full.fit(train, verbose=1)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Save the model\n",
    "import torch\n",
    "torch.save(model_full, 'meta_playlist_full')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load the validation data\n",
    "with open('Val_X.json', 'r') as f:\n",
    "    validation = json.load(f)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Generate the score dictionaries\n",
    "reverse_item_id = {}\n",
    "for key, value in item_id.items():\n",
    "    reverse_item_id[value] = key\n",
    "    \n",
    "def top_500_dic(inp):\n",
    "    dic = {}\n",
    "    score = np.array([0] * len(model.predict(1)))\n",
    "    sum_w = 0\n",
    "    for i in range(1,101):\n",
    "        s = model.predict(i, np.array(inp))\n",
    "        w = sum(s) / (np.linalg.norm(s) * np.sqrt(len(inp)))\n",
    "        sum_w += w\n",
    "        score = w * model.predict(i) + score\n",
    "    score = (score/sum_w)[1:]\n",
    "    for index in np.argsort(score)[-1:-501:-1]:\n",
    "        dic[reverse_item_id[index + 1]] = score[index] # map the song id in string format to numeric score\n",
    "    return dic\n",
    "\n",
    "rec_playlist = []\n",
    "for each_input in validation:\n",
    "    input_id = []\n",
    "    for i in range(len(each_input)):\n",
    "        it = each_input[i]\n",
    "        try: \n",
    "            input_id.append(item_id[it]) # Handle songs not in item_id\n",
    "        except:\n",
    "            pass\n",
    "    if len(input_id) == 1: # Model has to predict on more than one item\n",
    "        input_id.append(input_id[0])\n",
    "    if len(input_id) == 0: #If no known input, randomly select songs\n",
    "        input_id.append(1)\n",
    "        input_id.append(2)\n",
    "    rec_playlist.append(top_500_dic(input_id))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Output the results into a json file\n",
    "with open('score_500_full.json', 'w') as outfile:\n",
    "    json.dump(rec_playlist, outfile)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
